obj.fun  <- makeSingleObjectiveFunction(
fn=       funcion_optimizar,
minimize= FALSE,   #estoy Maximizando la ganancia
noisy=    TRUE,
par.set=  hs,
has.simple.signature = FALSE
)
ctrl  <- makeMBOControl( save.on.disk.at.time= 60,  save.file.path= kbayesiana)
ctrl  <- setMBOControlTermination(ctrl, iters= kBO_iter )
ctrl  <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI())
surr.km  <-  makeLearner("regr.km", predict.type= "se", covtype= "matern3_2", control= list(trace= TRUE))
#inicio la optimizacion bayesiana
if(!file.exists(kbayesiana)) {
run  <- mbo(obj.fun, learner = surr.km, control = ctrl)
} else  run  <- mboContinue( kbayesiana )   #retomo en caso que ya exista
install.packages("lightgbm")
library(lightgbm)
library(caret)
library(ggplot2)
load("/Users/achain/Downloads/E1000_rpart.RDATA")
rm( list=ls() )  #remove all objects
gc()             #garbage collection
load("/Users/achain/Downloads/E1000_rpart.RDATA")
View(opt.state)
load("/Users/achain/Downloads/E1000_rpart (1).RDATA")
View(opt.state)
View(opt.state)
opt.state$opt.path
opt.state$opt.path$env
opt.state$opt.path$env$dob
opt.state$opt.path$env$dob[1092]
opt.state$opt.path$env$dob[2000]
opt.state$opt.path$env$dob[2001]
opt.state$opt.path$env$dob[2012]
opt.state$opt.path$env$dob[2030]
opt.state$opt.path$env$dob[2016]
opt.state$opt.path$env$dob[2010]
opt.state$opt.path$env$dob[2016]
opt.state$opt.path$env$dob[202]
opt.state$opt.path$env$y
opt.state$opt.path$env$path
max(opt.state$opt.path$env$path[,y])
max(opt.state$opt.path$env$path[y])
opt.state$opt.path$env$path[y]
opt.state$opt.path$env$path[,"y"]
max(opt.state$opt.path$env$path[,"y"])
wich(max(opt.state$opt.path$env$path[,"y"]))
max.col(opt.state$opt.path$env$path[,"y"])
max(opt.state$opt.path$env$path[,"y"])
max(opt.state$opt.path$env$path["y"==max(opt.state$opt.path$env$path[,"y"])])
max(opt.state$opt.path$env$path["y"==max(opt.state$opt.path$env$path[,"y"]),])
opt.state$opt.path$env$path["y"==max(opt.state$opt.path$env$path[,"y"]
)
]
opt.state$opt.path$env$path["y"==22118000]
opt.state$opt.path$env$path[y==22118000]
opt.state$opt.path$env$path
opt.state$opt.path$env$path[,"y"]
opt.state$opt.path$env$path[,"y"]==22118000
opt.state$opt.path$env$path[opt.state$opt.path$env$path[,"y"]==22118000]
opt.state$opt.path$env$path[opt.state$opt.path$env$path[,"y"]==22118000,]
opt.state$opt.path$env$path[1192,]
modelo  <- rpart("clase_binaria ~ .",
data= dtrain[,-c("clase_ternaria","foto_mes")],
xval= 0,
cp=           -0.3616617,
minsplit=    1056,
minbucket=   119,
maxdepth=      7 )
#Archivo con datos etiquetados para entrenamiento
karchivo_entrada      <-  paste0(kcarpeta_datasets, "competencia1_2022.csv")
kcarpeta_datasets    <- "./datasets/"                          #VM o Ubuntu
#Archivo con datos etiquetados para entrenamiento
karchivo_entrada      <-  paste0(kcarpeta_datasets, "competencia1_2022.csv")
#cargo los datos
dataset <- fread(karchivo_entrada)
#creo la clase_binaria SI={ BAJA+1, BAJA+2 }    NO={ CONTINUA }
dataset[ foto_mes==202101,
clase_binaria :=  ifelse( clase_ternaria=="CONTINUA", "NO", "SI" ) ]
dtrain  <- dataset[ foto_mes==202101 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202103 ]  #defino donde voy a aplicar el modelo
modelo  <- rpart("clase_binaria ~ .",
data= dtrain[,-c("clase_ternaria","foto_mes")],
xval= 0,
cp=           -0.3616617,
minsplit=    1056,
minbucket=   119,
maxdepth=      7 )
#genero el vector con la prediccion, la probabilidad de ser positivo
prediccion  <- predict( modelo, dapply[,-c("clase_ternaria","foto_mes")])
prediccion  <- predict( modelo, dtrain[,-c("clase_ternaria","foto_mes")])
base=data.table(tr=dtrain[,clase_ternaria],prediccion)
base[tr=='BAJA+2']
base2<-base %>% pivot_longer(-tr)
base2 %>% filter(name=='SI') %>% ggplot(mapping = aes(x=value,col=tr)) + geom_density() + xlim(c(0,0.5))
library(tidyverse)
base2<-base %>% pivot_longer(-tr)
base2 %>% filter(name=='SI') %>% ggplot(mapping = aes(x=value,col=tr)) + geom_density() + xlim(c(0,0.5))
base2 %>% filter(name=='SI') %>% ggplot(mapping = aes(x=value,col=tr)) + geom_density() + xlim(c(0,0.1))
base2 %>% filter(name=='SI') %>% ggplot(mapping = aes(x=value,col=tr)) + geom_density() + xlim(c(0,0.4))
rm(list=ls())
require("data.table")
require("rpart")
require("rpart.plot")
require("data.table")
require("rpart")
require("rpart.plot")
file.choose()
#Aqui se debe poner la carpeta de la materia de SU computadora local
#setwd("~/buckets/b1/")  #Establezco el Working Directory
setwd("/Users/achain/Documents/github/labo/propio")  #Establezco el Working Directory
file.choose()
dataset  <- fread("/Users/achain/Downloads/competencia2_2022.csv.gz")  #donde entreno
dataset  <- dataset[  foto_mes %in% c( 202101, 202103 ) ]
#creo la clase_binaria SI={ BAJA+1, BAJA+2 }    NO={ CONTINUA }
dataset[ foto_mes==202101,
clase_binaria :=  ifelse( clase_ternaria=="CONTINUA", "NO", "SI" ) ]
# Entreno el modelo
# utilizo los mejores hiperparametros encontrados en una Bayesian Optimizationcon 5-fold Cross Validation
modelo  <- rpart(formula=   "clase_binaria ~ . -clase_ternaria",
data=      dataset[ foto_mes==202101 ],  #los datos donde voy a entrenar
xval=         0,
cp=           -0.69,
minsplit=    870,
minbucket=     9,
maxdepth=      9)
cantidad_hojas  <- sum( modelo$frame$var == "<leaf>" )
#aplico el modelo a TODOS los datos, incluso donde entreno
prediccion  <- predict( object=  modelo,
newdata= dataset,
type = "prob")
#le pego la prediccion al dataset
dataset[  , prob_SI := prediccion[ , "SI"]  ]
dataset[  , gan := ifelse( clase_ternaria=="BAJA+2", 78000, -2000 ) ]
tb_hojas  <-  dataset[  , list( reg_202101 = sum( ifelse( foto_mes==202101, 1, 0 ) ),
reg_202103 = sum( ifelse( foto_mes==202103, 1, 0 ) ),
b2_202101  = sum( ifelse( foto_mes==202101 & clase_ternaria=="BAJA+2", 1, 0 ) ),
b2_202103  = sum( ifelse( foto_mes==202103 & clase_ternaria=="BAJA+2", 1, 0 ) ),
gan_202101 = sum( ifelse( foto_mes==202101, gan, 0 ) ),
gan_202103 = sum( ifelse( foto_mes==202103, gan, 0 ) ) ),
by= prob_SI ]
View(tb_hojas)
#calculo las ganancias en 202101 y  202103, que es donde cae
tb_hojas[  gan_202101>0,
list( sum( gan_202101 ),  sum( gan_202103 ) ) ]
#dir.create( "./exp/",  showWarnings = FALSE )
#dir.create( "./exp/DR5120/", showWarnings = FALSE )
setwd("/Users/achain/Documents/github/labo/propio/exp")
setorder( tb_hojas,  -prob_SI )
tb_hojas[  , regacum_202101 := cumsum( reg_202101 ) ]
tb_hojas[  , regacum_202103 := cumsum( reg_202103 ) ]
tb_hojas[  , ganacum_202101 := cumsum( gan_202101 ) ]
tb_hojas[  , ganacum_202103 := cumsum( gan_202103 ) ]
View(tb_hojas)
fwrite( tb_hojas,
file= "tb_hojas.txt",
sep= "\t" )
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("rpart")
graficar_campo  <- function( campo, campo_clase, valores_clase )
{
#quito de grafico las colas del 5% de las densidades
qA  <- quantile(  dataset[ foto_mes==202101 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
qB  <- quantile(  dataset[ foto_mes==202103 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
xxmin  <- pmin( qA[[1]], qB[[1]] )
xxmax  <- pmax( qA[[2]], qB[[2]] )
densidad_A  <- density( dataset[ foto_mes==202101 & get(campo_clase) %in% valores_clase, get(campo) ],
kernel="gaussian", na.rm=TRUE )
densidad_B  <- density( dataset[ foto_mes==202103 & get(campo_clase) %in% valores_clase, get(campo) ],
kernel="gaussian", na.rm=TRUE )
plot( densidad_A,
col="blue",
xlim= c( xxmin, xxmax ),
ylim= c( 0, pmax( max(densidad_A$y), max(densidad_B$y) ) ),
main= paste0( campo, ",   ", campo_clase, " in ",  paste( valores_clase,collapse=","))
)
lines(densidad_B, col="red", lty=2)
legend(  "topright",
legend=c("202001", "202003"),
col=c("blue", "red"), lty=c(1,2))
}
#------------------------------------------------------------------------------
#Aqui comienza el programa
#setwd("~/buckets/b1")
setwd("/Users/achain/Documents/github/labo/propio")  #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("/Users/achain/Downloads/competencia2_2022.csv.gz")  #donde entreno
dataset  <- dataset[  foto_mes %in% c( 202101, 202103 ) ]
#creo la clase_binaria SI={ BAJA+1, BAJA+2 }    NO={ CONTINUA }
dataset[ foto_mes==202101,
clase_binaria :=  ifelse( clase_ternaria=="CONTINUA", "NO", "SI" ) ]
# Entreno el modelo
# utilizo los mejores hiperparametros encontrados en una Bayesian Optimizationcon 5-fold Cross Validation
modelo  <- rpart(formula=   "clase_binaria ~ . -clase_ternaria",
data=      dataset[ foto_mes==202101 ],  #los datos donde voy a entrenar
xval=         0,
cp=           -0.69,
minsplit=    870,
minbucket=     9,
maxdepth=      9)
campos_modelo  <- names( modelo$variable.importance )
campos_modelo
modelo$variable.importance
campos_buenos  <- c( campos_modelo,  setdiff( colnames(dataset), campos_modelo ) )
campos_buenos  <-  setdiff(  campos_buenos,  c( "foto_mes","clase_ternaria","clase_binaria" ) )
setdiff( colnames(dataset), campos_modelo )
setwd("/Users/achain/Documents/github/labo/propio/exp")
dir.create( "./DR5130", showWarnings = FALSE )
setwd("/Users/achain/Documents/github/labo/propio/exp/DR5130")
pdf("densidades_01_03.pdf")
for( campo in  campos_buenos )
for( campo in  campos_buenos )
{
cat( campo, "  " )
graficar_campo( campo, "clase_ternaria", c( "BAJA+1", "BAJA+2", "CONTINUA" ) )
graficar_campo( campo, "clase_ternaria", c( "BAJA+1", "BAJA+2" ) )
graficar_campo( campo, "clase_ternaria", c( "BAJA+2" ) )
graficar_campo( campo, "clase_ternaria", c( "BAJA+1" ) )
graficar_campo( campo, "clase_ternaria", c( "CONTINUA" ) )
}
dev.off()
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("rpart")
graficar_campo  <- function( campo, campo_clase, valores_clase )
{
#quito de grafico las colas del 5% de las densidades
qA  <- quantile(  dataset[ foto_mes not %in% (202103) , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
qB  <- quantile(  dataset[ foto_mes==202103 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
xxmin  <- pmin( qA[[1]], qB[[1]] )
xxmax  <- pmax( qA[[2]], qB[[2]] )
densidad_A  <- density( dataset[ foto_mes not %in% (202103) & get(campo_clase) %in% valores_clase, get(campo) ],
kernel="gaussian", na.rm=TRUE )
densidad_B  <- density( dataset[ foto_mes==202103 & get(campo_clase) %in% valores_clase, get(campo) ],
kernel="gaussian", na.rm=TRUE )
plot( densidad_A,
col="blue",
xlim= c( xxmin, xxmax ),
ylim= c( 0, pmax( max(densidad_A$y), max(densidad_B$y) ) ),
main= paste0( campo, ",   ", campo_clase, " in ",  paste( valores_clase,collapse=","))
)
lines(densidad_B, col="red", lty=2)
legend(  "topright",
legend=c("not 202003", "202003"),
col=c("blue", "red"), lty=c(1,2))
}
graficar_campo  <- function( campo, campo_clase, valores_clase )
{
#quito de grafico las colas del 5% de las densidades
qA  <- quantile(  dataset[ !(foto_mes  %in% (202103)) , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
qB  <- quantile(  dataset[ foto_mes==202103 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
xxmin  <- pmin( qA[[1]], qB[[1]] )
xxmax  <- pmax( qA[[2]], qB[[2]] )
densidad_A  <- density( dataset[ !(foto_mes  %in% (202103)) & get(campo_clase) %in% valores_clase, get(campo) ],
kernel="gaussian", na.rm=TRUE )
densidad_B  <- density( dataset[ foto_mes==202103 & get(campo_clase) %in% valores_clase, get(campo) ],
kernel="gaussian", na.rm=TRUE )
plot( densidad_A,
col="blue",
xlim= c( xxmin, xxmax ),
ylim= c( 0, pmax( max(densidad_A$y), max(densidad_B$y) ) ),
main= paste0( campo, ",   ", campo_clase, " in ",  paste( valores_clase,collapse=","))
)
lines(densidad_B, col="red", lty=2)
legend(  "topright",
legend=c("not 202003", "202003"),
col=c("blue", "red"), lty=c(1,2))
}
#cargo el dataset donde voy a entrenar
dataset  <- fread("/Users/achain/Documents/github/labo/propio/datasets/competencia2_2022.csv.gz")  #donde entreno
#cargo el dataset donde voy a entrenar
dataset  <- fread("/Users/achain/Downloads/competencia2_2022.csv.gz")  #donde entreno
#creo la clase_binaria SI={ BAJA+1, BAJA+2 }    NO={ CONTINUA }
dataset[ !(foto_mes  %in% (202103)) ,
clase_binaria :=  ifelse( clase_ternaria=="CONTINUA", "NO", "SI" ) ]
# Entreno el modelo
# utilizo los mejores hiperparametros encontrados en una Bayesian Optimizationcon 5-fold Cross Validation
modelo  <- rpart(formula=   "clase_binaria ~ . -clase_ternaria",
data=      dataset[ !(foto_mes  %in% (202103)) ],  #los datos donde voy a entrenar
xval=         0,
cp=           -0.69,
minsplit=    870,
minbucket=     9,
maxdepth=      9)
1/40
dataset[,foto_mes]
unique(dataset[,foto_mes])
count(unique(dataset[,foto_mes]))
len(unique(dataset[,foto_mes]))
length(unique(dataset[,foto_mes]))
require("data.table")
require("rpart")
require("rpart.plot")
file.choose()
#cargo el dataset de la competencia 2  , que tiene lo que paso en 202103
dataset  <- fread("/Users/achain/Downloads/competencia2_2022.csv.gz")  #donde entreno
#creo la clase_binaria SI={ BAJA+1, BAJA+2 }    NO={ CONTINUA }
dataset[ foto_mes==202101,
clase_binaria :=  ifelse( clase_ternaria=="CONTINUA", "NO", "SI" ) ]
# Entreno el modelo
# utilizo los mejores hiperparametros encontrados en una Bayesian Optimizationcon 5-fold Cross Validation
modelo  <- rpart(formula=   "clase_binaria ~ . -clase_ternaria",
data=      dataset[ foto_mes==202101 ],  #los datos donde voy a entrenar
xval=         0,
cp=           -0.69,
minsplit=    870,
minbucket=     9,
maxdepth=      9)
cantidad_hojas  <- sum( modelo$frame$var == "<leaf>" )
data.table(variable=names(modelo$variable.importance),feature_imp=modelo$variable.importance)
fwrite(data.table(variable=names(modelo$variable.importance),feature_imp=modelo$variable.importance),file = ("/Users/achain/Downloads/feature_imp_solodos.csv")
)
dataset[clase_ternaria=='Baja+2']
dataset[clase_ternaria=='BAJA+2']
dataset[clase_ternaria=='BAJA+2',n()]
dataset[clase_ternaria=='BAJA+2',count()]
dataset[clase_ternaria=='BAJA+2',count]
View(dataset)
head(dataset)
dataset[clase_ternaria=='BAJA+2',count(numero_de_cliente)]
dataset[clase_ternaria=='BAJA+2',count(as.character(numero_de_cliente))]
dataset[count(as.character(numero_de_cliente)),by=clase_ternaria]
dataset[,count(as.character(numero_de_cliente)),by=clase_ternaria]
dataset[,count(),by=clase_ternaria]
dataset[,N,by=clase_ternaria]
dataset[.N,by=clase_ternaria]
dataset[.N,by=clase_ternaria]
dataset[numero_de_cliente,.N,by=clase_ternaria]
dataset[numero_de_cliente,.N,by=clase_ternaria]
dataset[on=numero_de_cliente,.N,by=clase_ternaria]
dataset[numero_de_cliente,.N,by=clase_ternaria]
dataset[numero_de_cliente,.N]
dataset[numero_de_cliente]
names(dataaset)
names(dataset)
dataset[numero_de_cliente]
dataset['numero_de_cliente']
dataset[,.N]
dataset[,.N,by=clase_ternaria]
dataset[foto_mes=='202203',.N,by=clase_ternaria]
dataset[foto_mes=='202203']
dataset[foto_mes=='202103',.N,by=clase_ternaria]
install.packages('ranger')
install.packages('caret')
install.packages("caret")
library("data.table")
library("ROCR")
library("caret")
library("ranger")
#limpio la memoria
rm(list=ls())
gc()
library("data.table")
library("ROCR")
library("caret")
library("ranger")
#kcarpeta_datasets    <- "../input/laboratorio-de-implementacion-i-2021/"   #KAGGLE
kcarpeta_datasets    <- "/Users/achain/Documents/github/labo/propio/datasets/"                          #VM o Ubuntu
#Archivo con datos etiquetados para entrenamiento
karchivo_entrada      <-  paste0(kcarpeta_datasets, "competencia1_2022_FE.csv")
#Separador de campos en archivos
kcampos_separador     <-  "\t"
#Campo que identifica las muestras
kcampo_id             <-  "numero_de_cliente"
#Campo que contiene la clase a estimar
kclase_nomcampo       <-  "clase_ternaria"
#Valor de interés
kclase_valor_positivo <-  "BAJA+2"
#Campos a borrar para el entrenamiento
kcampos_a_borrar      <-  c(kcampo_id,kclase_nomcampo,"foto_mes")
#Campo que contendrá a la variable objetivo generada
kobjetivo             <-  "clase"
#Identificación del modelo
kmodelo               <-  "02-RPART"
#Ganancia por TP
kTPGain               <-  78000
#Pérdida por FP
kFPGain               <-  -2000
#Establezco semilla aleatoria
set.seed(1)
#Genero la clase
dataset[,'clase' := as.integer(dataset[,'clase_ternaria'] == 'BAJA+2')]
#cargo los datos
dataset <- fread(karchivo_entrada)
#Genero la clase
dataset[,'clase' := as.integer(dataset[,'clase_ternaria'] == 'BAJA+2')]
#Limpio campos a borrar
dataset[ ,  'clase_ternaria' := NULL    ]
#Completo missings
dataset[is.na(dataset)] <- 0
dtrain  <- dataset[ foto_mes==202101 ]  #defino donde voy a entrenar
train_rows <- createDataPartition(dtrain$clase, p = .66, list = FALSE)
#train_rows <- sample(1:nrow(dataset), .66 * nrow(dataset))
dtest <- dtrain[-train_rows,]
dtrain_split <- dtrain[train_rows,]
dapply  <- dataset[ foto_mes==202103 ]  #defino donde voy a aplicar el modelo
install.packages('caret')
rm(list=ls())
gc()
library("data.table")
library("ROCR")
library("caret")
library("ranger")
para poder usarlo en la PC y en la nube
#switch ( Sys.info()[['sysname']],
#        Windows = { directory.root   <-  "M:\\" },   #Microsoft Windows
#       Darwin  = { directory.root   <-  "~/CloudL/UA/labo2022/" },  #Apple MAC
#       Linux   = { directory.root   <-  "~/buckets/b1/" }  #Entorno Google Cloud
#    )
#defino la carpeta donde trabajo
#setwd( directory.root )
#setwd("~/github/labo/propio/datasets")
#Parametros entrada
#kcarpeta_datasets    <- "../input/laboratorio-de-implementacion-i-2021/"   #KAGGLE
kcarpeta_datasets    <- "/Users/achain/Documents/github/labo/propio/datasets/"                          #VM o Ubuntu
#Archivo con datos etiquetados para entrenamiento
karchivo_entrada      <-  paste0(kcarpeta_datasets, "competencia1_2022_FE.csv")
#Formato para submit en Kaggle
#karchivo_score      <-  "../input/uamds2020ldi1f1/Sample_201910_Fase_I.txt"
#Separador de campos en archivos
kcampos_separador     <-  "\t"
#Campo que identifica las muestras
kcampo_id             <-  "numero_de_cliente"
#Campo que contiene la clase a estimar
kclase_nomcampo       <-  "clase_ternaria"
#Valor de interés
kclase_valor_positivo <-  "BAJA+2"
#Campos a borrar para el entrenamiento
kcampos_a_borrar      <-  c(kcampo_id,kclase_nomcampo,"foto_mes")
#Campo que contendrá a la variable objetivo generada
kobjetivo             <-  "clase"
#Identificación del modelo
kmodelo               <-  "02-RPART"
#Ganancia por TP
kTPGain               <-  78000
#Pérdida por FP
kFPGain               <-  -2000
#Establezco semilla aleatoria
set.seed(1)
#cargo los datos
dataset <- fread(karchivo_entrada)
#Para hacer pruebas rapidas puedo reducir el dataset a una fraccion
#subsample <- sample(1:nrow(dataset), .1 * nrow(dataset))
#dataset <- dataset[subsample,]
#Genero la clase
dataset[,'clase' := as.integer(dataset[,'clase_ternaria'] == 'BAJA+2')]
#Limpio campos a borrar
dataset[ ,  'clase_ternaria' := NULL    ]
#Completo missings
dataset[is.na(dataset)] <- 0
dtrain  <- dataset[ foto_mes==202101 ]  #defino donde voy a entrenar
train_rows <- createDataPartition(dtrain$clase, p = .66, list = FALSE)
#train_rows <- sample(1:nrow(dataset), .66 * nrow(dataset))
dtest <- dtrain[-train_rows,]
dtrain_split <- dtrain[train_rows,]
dapply  <- dataset[ foto_mes==202103 ]  #defino donde voy a aplicar el modelo
# generacion del modelo
formula_obj  <-  formula(paste("clase ~ ."))
formula_obj  <-  formula(paste("clase ~ ."))
t0       <-  Sys.time()
modelo  <- ranger( data = dtrain_split,
formula_obj,
probability=TRUE,
num.trees= 900,
min.node.size= 360,
mtry= 4,
splitrule='gini'
)
t1       <-  Sys.time()
tcorrida <-  as.numeric( t1 - t0, units = "secs")
print( tcorrida)
#Calculo ganancia sobre dataset de pruebas
dtest$score <- predict(modelo, dtest, type = 'response')$predictions[,2]
#Selecciono los que estan por encima del umbral
dtest$Predicted <- dtest$score > 1/40
#Determino ganancia
print(paste("La Ganancia es:", 3*sum(dtest$Predicted*dtest[,clase]*kTPGain +
dtest$Predicted * (0 == dtest[,clase]) * kFPGain)))
install.packages('ROCR')
#ROC con curvas de nivel de ganancia
pred <- prediction(dtest$score, dtest[,clase])
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
performance( pred,"auc")@y.values
tot_positivos = sum( dtest[,clase] )
tot_negativos = nrow(dtest) - sum( dtest[,clase] )
options(repr.plot.width=15, repr.plot.height=15)
plot.new()
plot(perf,
add = TRUE,
lwd=.7,
main="ROC Curves",
avg = "vertical")
grid(col="lightgray")
axis(1, at=seq(0, 1, by=0.1))
axis(2, at=seq(0, 1, by=0.1))
abline(v=c(0.1, 0.3, 0.5, 0.7, 0.9), col="lightgray", lty="dotted")
abline(h=c(0.1, 0.3, 0.5, 0.7, 0.9), col="lightgray", lty="dotted")
lines(x=c(0, 1), y=c(0, 1), col="black", lty="dotted")
par( new=TRUE)
x <- seq(0, 1, len = 1500)
y <- seq(0, 1, len = 1500)
r <- outer(tot_negativos*x*kFPGain *3/1000000,tot_positivos*y*(kTPGain )*3/1000000,"+")
contour(r, axes = FALSE, nlevels = 15)
#Determino ganancia
print(paste("La Ganancia es:", 3*sum(dtest$Predicted*dtest[,clase]*kTPGain +
dtest$Predicted * (0 == dtest[,clase]) * kFPGain)))
